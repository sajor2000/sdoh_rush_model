#!/usr/bin/env python3
"""
Fixed version - Refit SDOH Model with REAL DATA using proper calibration
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import (
    roc_auc_score, average_precision_score, brier_score_loss,
    confusion_matrix
)
import xgboost as xgb
import joblib
import json
import os
from datetime import datetime
import warnings
import time

warnings.filterwarnings('ignore')

# Set random seed
RANDOM_SEED = 2025
np.random.seed(RANDOM_SEED)

# Optimize for Apple Silicon
os.environ['OMP_NUM_THREADS'] = '16'
os.environ['MKL_NUM_THREADS'] = '16'

print("üñ•Ô∏è  SDOH Model Training with REAL DATA")
print("=" * 60)
print(f"   CPU Cores: 16 (Apple M4 Max)")
print(f"   Random Seed: {RANDOM_SEED}")
print("=" * 60)

class CalibratedSDOHModel:
    """SDOH prediction model with calibration"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.base_model = None
        self.calibrated_model = None
        self.metrics = {}
        self.n_jobs = 16
        
    def calculate_ece(self, y_true, y_pred_proba, n_bins=10):
        """Calculate Expected Calibration Error"""
        bin_boundaries = np.linspace(0, 1, n_bins + 1)
        bin_lowers = bin_boundaries[:-1]
        bin_uppers = bin_boundaries[1:]
        
        ece = 0
        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
            in_bin = (y_pred_proba > bin_lower) & (y_pred_proba <= bin_upper)
            prop_in_bin = in_bin.mean()
            
            if prop_in_bin > 0:
                accuracy_in_bin = y_true[in_bin].mean()
                avg_confidence_in_bin = y_pred_proba[in_bin].mean()
                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        
        return ece
    
    def calculate_metrics(self, y_true, y_pred_proba, threshold=0.5644):
        """Calculate comprehensive metrics"""
        y_pred = (y_pred_proba >= threshold).astype(int)
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
        npv = tn / (tn + fn) if (tn + fn) > 0 else 0
        
        ece = self.calculate_ece(y_true, y_pred_proba)
        brier = brier_score_loss(y_true, y_pred_proba)
        auc = roc_auc_score(y_true, y_pred_proba)
        auprc = average_precision_score(y_true, y_pred_proba)
        
        # Calculate screening rate
        screening_rate = (y_pred == 1).mean()
        
        return {
            'auc': float(auc),
            'auprc': float(auprc),
            'ece': float(ece),
            'brier_score': float(brier),
            'sensitivity': float(sensitivity),
            'specificity': float(specificity),
            'ppv': float(ppv),
            'npv': float(npv),
            'screening_rate': float(screening_rate),
            'threshold': float(threshold)
        }
    
    def fit_with_cross_validation(self, X_train, y_train, X_val, y_val):
        """Fit model with cross-validation focused on calibration"""
        print("\nüîÑ Starting cross-validation with REAL data...")
        start_time = time.time()
        
        # Best parameters based on your original model
        best_params = {
            'max_depth': 3,  # Keep shallow for interpretability
            'learning_rate': 0.1,
            'n_estimators': 200,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 5,
            'gamma': 0.1,
            'scale_pos_weight': 1,  # Let XGBoost handle imbalance naturally
            'objective': 'binary:logistic',
            'eval_metric': 'logloss',
            'seed': self.random_state,
            'tree_method': 'hist',
            'n_jobs': self.n_jobs
        }
        
        print("\nüéØ Training model with proven parameters...")
        self.base_model = xgb.XGBClassifier(**best_params)
        
        # Train with early stopping
        eval_set = [(X_train, y_train), (X_val, y_val)]
        self.base_model.fit(
            X_train, y_train,
            eval_set=eval_set,
            verbose=True
        )
        
        # Get base model predictions for analysis
        y_val_pred = self.base_model.predict_proba(X_val)[:, 1]
        base_ece = self.calculate_ece(y_val, y_val_pred)
        print(f"\nüìä Base model ECE on validation: {base_ece:.4f}")
        
        # Apply calibration only if needed
        if base_ece > 0.05:
            print("\nüìä Applying Platt scaling calibration...")
            self.calibrated_model = CalibratedClassifierCV(
                self.base_model, 
                method='sigmoid',
                cv='prefit'
            )
            self.calibrated_model.fit(X_val, y_val)
        else:
            print("\n‚úÖ Model already well calibrated, skipping Platt scaling")
            self.calibrated_model = self.base_model
        
        self.best_params = best_params
        
        return self
    
    def evaluate(self, X_test, y_test):
        """Evaluate both models"""
        print(f"\nüìà Evaluating on test set ({len(y_test):,} samples)...")
        
        # Get predictions
        y_pred_base = self.base_model.predict_proba(X_test)[:, 1]
        if hasattr(self.calibrated_model, 'predict_proba'):
            y_pred_calibrated = self.calibrated_model.predict_proba(X_test)[:, 1]
        else:
            y_pred_calibrated = y_pred_base
        
        # Calculate metrics at recommended threshold
        metrics_base = self.calculate_metrics(y_test, y_pred_base, threshold=0.5644)
        metrics_calibrated = self.calculate_metrics(y_test, y_pred_calibrated, threshold=0.5644)
        
        # Also calculate at optimal threshold for comparison
        from sklearn.metrics import precision_recall_curve
        precision, recall, thresholds = precision_recall_curve(y_test, y_pred_calibrated)
        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)
        optimal_idx = np.argmax(f1_scores[:-1])  # Last value is always 1.0
        optimal_threshold = thresholds[optimal_idx]
        metrics_optimal = self.calculate_metrics(y_test, y_pred_calibrated, threshold=optimal_threshold)
        
        self.metrics = {
            'base_model': metrics_base,
            'calibrated_model': metrics_calibrated,
            'optimal_threshold': metrics_optimal,
            'optimal_threshold_value': float(optimal_threshold),
            'improvement': {
                'ece': (metrics_base['ece'] - metrics_calibrated['ece']) / (metrics_base['ece'] + 1e-7) * 100,
                'brier': (metrics_base['brier_score'] - metrics_calibrated['brier_score']) / (metrics_base['brier_score'] + 1e-7) * 100
            }
        }
        
        # Print results
        print("\nüìä REAL DATA Results:")
        print("=" * 70)
        print(f"Metric          | Base Model | Calibrated | At Optimal Threshold")
        print("-" * 70)
        
        for metric in ['auc', 'auprc', 'ece', 'brier_score', 'sensitivity', 'specificity', 'ppv', 'screening_rate']:
            base_val = metrics_base.get(metric, 0)
            cal_val = metrics_calibrated.get(metric, 0)
            opt_val = metrics_optimal.get(metric, 0)
            
            print(f"{metric:15} | {base_val:.4f}     | {cal_val:.4f}     | {opt_val:.4f}")
        
        print(f"\nOptimal threshold: {optimal_threshold:.4f} (vs recommended: 0.5644)")
        
        # Clinical summary
        print(f"\nüè• Clinical Impact Summary (at 0.5644 threshold):")
        print(f"   ‚Ä¢ Screening rate: {metrics_calibrated['screening_rate']:.1%} of patients")
        print(f"   ‚Ä¢ PPV: {metrics_calibrated['ppv']:.1%} (vs {y_test.mean():.1%} baseline)")
        print(f"   ‚Ä¢ Sensitivity: {metrics_calibrated['sensitivity']:.1%} of true positives caught")
        print(f"   ‚Ä¢ Specificity: {metrics_calibrated['specificity']:.1%}")
        print(f"   ‚Ä¢ Calibration: ECE = {metrics_calibrated['ece']:.4f} {'‚úÖ Excellent' if metrics_calibrated['ece'] < 0.05 else '‚ö†Ô∏è  Needs improvement'}")
        
        return self.metrics
    
    def save_models(self, output_dir='models'):
        """Save models with metadata"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Save models
        base_path = os.path.join(output_dir, 'xgboost_realdata_base.json')
        self.base_model.save_model(base_path)
        
        calibrated_path = os.path.join(output_dir, 'xgboost_realdata_calibrated.joblib')
        joblib.dump(self.calibrated_model, calibrated_path)
        
        # Convert numpy types to Python types for JSON serialization
        def convert_to_python_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_to_python_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_python_types(v) for v in obj]
            return obj
        
        # Save comprehensive metadata
        metadata = {
            'created_date': datetime.now().isoformat(),
            'data_source': 'REAL SDOH DATA (sdoh2_ml_final_all_svi.csv)',
            'random_seed': RANDOM_SEED,
            'data_split': '60/20/20',
            'n_samples': {
                'total': 393725,
                'train': 236235,
                'calibration': 78745,
                'test': 78745
            },
            'best_params': convert_to_python_types(self.best_params),
            'metrics': convert_to_python_types(self.metrics),
            'clinical_notes': {
                'recommended_threshold': 0.5644,
                'optimal_threshold': self.metrics.get('optimal_threshold_value', 0.5),
                'target_prevalence': 0.066,
                'calibration_method': 'Platt scaling (if ECE > 0.05)'
            }
        }
        
        metadata_path = os.path.join(output_dir, 'realdata_model_metadata.json')
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=4)
        
        print(f"\n‚úÖ Models saved to {output_dir}/")
        
        return base_path, calibrated_path, metadata_path


def load_real_sdoh_data():
    """Load the actual SDOH dataset"""
    print("\nüìä Loading REAL SDOH data...")
    
    # Path to the real dataset
    data_path = '/Users/jcrmacstudio/Desktop/vebe coding/SDOH_model/sdoh2_ml_final_all_svi.csv'
    
    # Load data
    df = pd.read_csv(data_path)
    print(f"   Loaded {len(df):,} samples")
    
    # Prepare features and target
    # Exclude first 3 columns as per previous analysis
    feature_cols = df.columns[3:].tolist()  # All columns after sdoh_two_yes
    
    X = df[feature_cols]
    y = df['sdoh_two_yes']  # Target variable
    
    print(f"   Features: {len(feature_cols)} columns")
    print(f"   Target prevalence: {y.mean():.1%} (expecting ~6.6%)")
    print(f"   Sample features: {feature_cols[:5]} ...")
    
    return X, y


def main():
    """Main function with real data"""
    print("üöÄ SDOH Model Training - REAL DATA")
    print("=" * 60)
    
    overall_start = time.time()
    
    # Load REAL data
    X, y = load_real_sdoh_data()
    
    # Create 60/20/20 split
    print("\nüîÄ Creating 60/20/20 split...")
    
    # First split: 80/20 for (train+cal)/test
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED
    )
    
    # Second split: 75/25 of the 80% for train/cal (= 60/20 of total)
    X_train, X_cal, y_train, y_cal = train_test_split(
        X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=RANDOM_SEED
    )
    
    print(f"   Training: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)")
    print(f"   Calibration: {len(X_cal):,} samples ({len(X_cal)/len(X)*100:.1f}%)")
    print(f"   Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)")
    print(f"   Prevalence - Train: {y_train.mean():.2%}, Cal: {y_cal.mean():.2%}, Test: {y_test.mean():.2%}")
    
    # Initialize and train model
    model = CalibratedSDOHModel(random_state=RANDOM_SEED)
    
    # Fit with cross-validation
    model.fit_with_cross_validation(X_train, y_train, X_cal, y_cal)
    
    # Evaluate on test set
    metrics = model.evaluate(X_test, y_test)
    
    # Save models
    model.save_models()
    
    # Feature importance analysis
    print("\nüìä Top 10 Feature Importances:")
    feature_importance = model.base_model.feature_importances_
    feature_names = X.columns
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False).head(10)
    
    for idx, row in importance_df.iterrows():
        print(f"   {row['feature']:30} : {row['importance']:.4f}")
    
    total_time = time.time() - overall_start
    
    print("\n" + "=" * 60)
    print("üéâ REAL DATA MODEL TRAINING COMPLETE!")
    print("=" * 60)
    print(f"Total runtime: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
    print(f"\nüéØ Key Results with REAL DATA:")
    print(f"   ‚Ä¢ AUC: {metrics['calibrated_model']['auc']:.4f}")
    print(f"   ‚Ä¢ AUPRC: {metrics['calibrated_model']['auprc']:.4f}")
    print(f"   ‚Ä¢ ECE: {metrics['calibrated_model']['ece']:.4f}")
    print(f"   ‚Ä¢ PPV @ 0.5644: {metrics['calibrated_model']['ppv']:.4f}")
    print(f"   ‚Ä¢ Sensitivity @ 0.5644: {metrics['calibrated_model']['sensitivity']:.4f}")
    
    print("\nüìã This is your production-ready model trained on REAL patient data!")

if __name__ == "__main__":
    main()